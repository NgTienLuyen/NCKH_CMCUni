{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/Q4DnPGSxfKhpJ99GjU8s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Simple audio recognition: Recognizing keywords"],"metadata":{"id":"mvoNV_YWEz-J"}},{"cell_type":"markdown","source":["#Setup\n","Nhập các mô-đun và phụ thuộc cần thiết. Bạn sẽ sử dụng tf.keras.utils.audio_dataset_from_directory (được giới thiệu trong TensorFlow 2.10), giúp tạo tập dữ liệu phân loại âm thanh từ các thư mục của tệp .wav. Bạn cũng sẽ cần seaborn để hình dung trong hướng dẫn này."],"metadata":{"id":"Fe3R90Z8FMK0"}},{"cell_type":"code","source":["!pip install -U -q tensorflow tensorflow_datasets"],"metadata":{"id":"Lpvtmg5_Eg8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pathlib\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from IPython import display\n","\n","# Set the seed value for experiment reproducibility.\n","seed = 42\n","tf.random.set_seed(seed)\n","np.random.seed(seed)"],"metadata":{"id":"pmCG4lKUGRy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"xQo0ArtjGX8u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import the mini Speech Commands dataset\n"],"metadata":{"id":"EZJabag7GYl4"}},{"cell_type":"code","source":["DATASET_PATH = '/content/drive/MyDrive/VOICE/newdata'\n","\n","data_dir = pathlib.Path(DATASET_PATH)\n","if not data_dir.exists():\n","  tf.keras.utils.get_file(\n","      'mini_speech_commands.zip',\n","      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n","      extract=True,\n","      cache_dir='.', cache_subdir='data')"],"metadata":{"id":"V1tCzfJkGffw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n","commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n","print('Commands:', commands)"],"metadata":{"id":"h7AAWS8eGpm5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Được chia thành các thư mục theo cách này, bạn có thể dễ dàng tải dữ liệu bằng keras.utils.audio_dataset_from_directory.\n","\n","Các clip âm thanh có thời lượng từ 1 giây trở xuống ở tần số 16kHz. Output_sequence_length=16000 đệm những cái ngắn đến chính xác 1 giây (và sẽ cắt bớt những cái dài hơn) để chúng có thể dễ dàng được phân nhóm."],"metadata":{"id":"MZW6nrtSHlvu"}},{"cell_type":"code","source":["train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n","    directory=data_dir,\n","    batch_size=64,\n","    validation_split=0.2,\n","    seed=0,\n","    output_sequence_length=16000,\n","    subset='both')\n","\n","label_names = np.array(train_ds.class_names)\n","print()\n","print(\"label names:\", label_names)"],"metadata":{"id":"ppshuKfzHmm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tập dữ liệu hiện chứa nhiều clip âm thanh và nhãn số nguyên. Các clip âm thanh có hình dạng  (batch, samples, channels)."],"metadata":{"id":"EMs8lRWRHu5j"}},{"cell_type":"code","source":["train_ds.element_spec"],"metadata":{"id":"ZKOj6moSHyTO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tập dữ liệu này chỉ chứa âm thanh một kênh, vì vậy hãy sử dụng hàm tf.squeeze để loại bỏ trục phụ:"],"metadata":{"id":"HqHT_7pZH1vm"}},{"cell_type":"code","source":["def squeeze(audio, labels):\n","  audio = tf.squeeze(audio, axis=-1)\n","  return audio, labels\n","\n","train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n","val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"],"metadata":{"id":"9qLvell_H0t0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hàm utils.audio_dataset_from_directory chỉ trả về tối đa hai phần tách. Bạn nên tách biệt bộ kiểm tra với bộ xác thực của mình. Lý tưởng nhất là bạn nên giữ nó trong một thư mục riêng, nhưng trong trường hợp này, bạn có thể sử dụng Dataset.shard để chia bộ xác thực thành hai nửa. Lưu ý rằng việc lặp lại bất kỳ phân đoạn nào sẽ tải tất cả dữ liệu và chỉ giữ lại phần của nó."],"metadata":{"id":"COMWXIvbIIov"}},{"cell_type":"code","source":["test_ds = val_ds.shard(num_shards=2, index=0)\n","val_ds = val_ds.shard(num_shards=2, index=1)"],"metadata":{"id":"O6_TRAp5ICOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for example_audio, example_labels in train_ds.take(1):\n","  print(example_audio.shape)\n","  print(example_labels.shape)"],"metadata":{"id":"Mm-SxerUIOru"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hãy vẽ một vài dạng sóng âm thanh:"],"metadata":{"id":"CO4jb2y9IYLe"}},{"cell_type":"code","source":["plt.figure(figsize=(16, 10))\n","rows = 3\n","cols = 3\n","n = rows * cols\n","for i in range(n):\n","  plt.subplot(rows, cols, i+1)\n","  audio_signal = example_audio[i]\n","  plt.plot(audio_signal)\n","  plt.title(label_names[example_labels[i]])\n","  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n","  plt.ylim([-1.1, 1.1])"],"metadata":{"id":"xb-Kftc7IQOC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Convert waveforms to spectrograms\n","Các dạng sóng trong tập dữ liệu được biểu diễn trong miền thời gian. Tiếp theo, bạn sẽ chuyển đổi dạng sóng từ tín hiệu miền thời gian thành tín hiệu miền tần số thời gian bằng cách tính toán biến đổi Fourier thời gian ngắn (STFT) để chuyển đổi dạng sóng thành biểu đồ phổ, hiển thị sự thay đổi tần số theo thời gian và có thể được được biểu diễn dưới dạng hình ảnh 2D. Bạn sẽ đưa các hình ảnh phổ vào mạng lưới thần kinh của mình để huấn luyện mô hình.\n","\n","Biến đổi Fourier (tf.signal.fft) chuyển đổi tín hiệu thành các tần số thành phần của nó nhưng mất toàn bộ thông tin về thời gian. Để so sánh, STFT (tf.signal.stft) chia tín hiệu thành các cửa sổ thời gian và chạy biến đổi Fourier trên mỗi cửa sổ, lưu giữ một số thông tin về thời gian và trả về một tenxơ 2D mà bạn có thể chạy các tích chập tiêu chuẩn trên đó.\n","\n","Tạo hàm tiện ích để chuyển đổi dạng sóng thành biểu đồ phổ:\n","\n","Các dạng sóng cần phải có cùng độ dài để khi bạn chuyển đổi chúng thành biểu đồ phổ, kết quả có kích thước tương tự nhau. Điều này có thể được thực hiện bằng cách đơn giản thêm khoảng đệm vào các clip âm thanh ngắn hơn một giây (sử dụng tf.zeros).\n","Khi gọi tf.signal.stft, hãy chọn tham số frame_length và frame_step sao cho \"hình ảnh\" phổ được tạo gần như hình vuông. Để biết thêm thông tin về lựa chọn tham số STFT, hãy tham khảo video Coursera này về xử lý tín hiệu âm thanh và STFT.\n","STFT tạo ra một dãy số phức biểu thị cường độ và pha. Tuy nhiên, trong hướng dẫn này, bạn sẽ chỉ sử dụng độ lớn mà bạn có thể rút ra bằng cách áp dụng tf.abs trên đầu ra của tf.signal.stft."],"metadata":{"id":"uBtZXBrZIjIs"}},{"cell_type":"code","source":["def get_spectrogram(waveform):\n","  # Convert the waveform to a spectrogram via a STFT.\n","  spectrogram = tf.signal.stft(\n","      waveform, frame_length=255, frame_step=128)\n","  # Obtain the magnitude of the STFT.\n","  spectrogram = tf.abs(spectrogram)\n","  # Add a `channels` dimension, so that the spectrogram can be used\n","  # as image-like input data with convolution layers (which expect\n","  # shape (`batch_size`, `height`, `width`, `channels`).\n","  spectrogram = spectrogram[..., tf.newaxis]\n","  return spectrogram"],"metadata":{"id":"6Vqs98qDIi8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio"],"metadata":{"id":"9K0LtFRSI_lL"}},{"cell_type":"code","source":["for i in range(3):\n","  label = label_names[example_labels[i]]\n","  waveform = example_audio[i]\n","  spectrogram = get_spectrogram(waveform)\n","\n","  print('Label:', label)\n","  print('Waveform shape:', waveform.shape)\n","  print('Spectrogram shape:', spectrogram.shape)\n","  print('Audio playback')\n","  display.display(display.Audio(waveform, rate=16000))"],"metadata":{"id":"SRM8Y3T9I4Aq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bây giờ, hãy xác định hàm để hiển thị biểu đồ phổ:"],"metadata":{"id":"XVtgNHliJHAm"}},{"cell_type":"code","source":["def plot_spectrogram(spectrogram, ax):\n","  if len(spectrogram.shape) > 2:\n","    assert len(spectrogram.shape) == 3\n","    spectrogram = np.squeeze(spectrogram, axis=-1)\n","  # Convert the frequencies to log scale and transpose, so that the time is\n","  # represented on the x-axis (columns).\n","  # Add an epsilon to avoid taking a log of zero.\n","  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n","  height = log_spec.shape[0]\n","  width = log_spec.shape[1]\n","  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n","  Y = range(height)\n","  ax.pcolormesh(X, Y, log_spec)"],"metadata":{"id":"zbJaNHc_JCGQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"],"metadata":{"id":"pcW641zPJeuQ"}},{"cell_type":"code","source":["fig, axes = plt.subplots(2, figsize=(12, 8))\n","timescale = np.arange(waveform.shape[0])\n","axes[0].plot(timescale, waveform.numpy())\n","axes[0].set_title('Waveform')\n","axes[0].set_xlim([0, 16000])\n","\n","plot_spectrogram(spectrogram.numpy(), axes[1])\n","axes[1].set_title('Spectrogram')\n","plt.suptitle(label.title())\n","plt.show()"],"metadata":{"id":"nua--wMNJKoV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bây giờ, hãy tạo bộ dữ liệu biểu đồ phổ từ bộ dữ liệu âm thanh:"],"metadata":{"id":"w1bg_Dt0JycH"}},{"cell_type":"code","source":["def make_spec_ds(ds):\n","  return ds.map(\n","      map_func=lambda audio,label: (get_spectrogram(audio), label),\n","      num_parallel_calls=tf.data.AUTOTUNE)"],"metadata":{"id":"q1XxyhzhJzm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_spectrogram_ds = make_spec_ds(train_ds)\n","val_spectrogram_ds = make_spec_ds(val_ds)\n","test_spectrogram_ds = make_spec_ds(test_ds)"],"metadata":{"id":"NhKLSqkYJ2IH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Kiểm tra các biểu đồ phổ cho các ví dụ khác nhau của tập dữ liệu:"],"metadata":{"id":"Rv4hMUwCJ8e4"}},{"cell_type":"code","source":["for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n","  break"],"metadata":{"id":"pN8Ccg6KJ8Ny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rows = 3\n","cols = 3\n","n = rows*cols\n","fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n","\n","for i in range(n):\n","    r = i // cols\n","    c = i % cols\n","    ax = axes[r][c]\n","    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n","    ax.set_title(label_names[example_spect_labels[i].numpy()])\n","\n","plt.show()"],"metadata":{"id":"CwXLE-HUJ_Eg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Build and train the model"],"metadata":{"id":"w64Eyv2jKDOV"}},{"cell_type":"markdown","source":["Thêm các thao tác `Dataset.cache` và `Dataset.prefetch` để giảm độ trễ đọc trong khi đào tạo mô hình:"],"metadata":{"id":"4WgIuwjcKDLC"}},{"cell_type":"code","source":["train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n","val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n","test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"Of_rbY4cKA3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Đối với mô hình này, bạn sẽ sử dụng mạng thần kinh tích chập (CNN) đơn giản vì bạn đã chuyển đổi các tệp âm thanh thành hình ảnh phổ.\n","\n","Mô hình `tf.keras.Sequential` của bạn sẽ sử dụng các lớp tiền xử lý Keras sau:\n","\n","- `tf.keras.layers.Resizing`: giảm mẫu đầu vào để cho phép mô hình đào tạo nhanh hơn.\n","- `tf.keras.layers.Normalization`: để chuẩn hóa từng pixel trong ảnh dựa trên giá trị trung bình và độ lệch chuẩn của nó.\n","\n","Đối với lớp `Chuẩn hóa`, phương pháp `thích ứng` của nó trước tiên cần được gọi trên dữ liệu huấn luyện để tính toán số liệu thống kê tổng hợp (nghĩa là giá trị trung bình và độ lệch chuẩn)."],"metadata":{"id":"I1nq17QLKXXi"}},{"cell_type":"code","source":["input_shape = example_spectrograms.shape[1:]\n","print('Input shape:', input_shape)\n","num_labels = len(label_names)\n","\n","# Instantiate the `tf.keras.layers.Normalization` layer.\n","norm_layer = layers.Normalization()\n","# Fit the state of the layer to the spectrograms\n","# with `Normalization.adapt`.\n","norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n","\n","model = models.Sequential([\n","    layers.Input(shape=input_shape),\n","    # Downsample the input.\n","    layers.Resizing(32, 32),\n","    # Normalize.\n","    norm_layer,\n","    layers.Conv2D(32, 3, activation='relu'),\n","    layers.Conv2D(64, 3, activation='relu'),\n","    layers.MaxPooling2D(),\n","    layers.Dropout(0.25),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.5),\n","    layers.Dense(num_labels),\n","])\n","\n","model.summary()"],"metadata":{"id":"Sa0LZjv1Kclx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Định cấu hình mô hình Keras với trình tối ưu hóa Adam và mất entropy chéo:"],"metadata":{"id":"mKwtPWJyKkik"}},{"cell_type":"code","source":["model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=['accuracy'],\n",")"],"metadata":{"id":"Czbqzwv2KlSR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the model over 10 epochs for demonstration purposes:"],"metadata":{"id":"JxzTtcpnKq-Y"}},{"cell_type":"code","source":["EPOCHS = 10\n","history = model.fit(\n","    train_spectrogram_ds,\n","    validation_data=val_spectrogram_ds,\n","    epochs=EPOCHS,\n","    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",")"],"metadata":{"id":"Ax3SUW7pKnZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = history.history\n","plt.figure(figsize=(16,6))\n","plt.subplot(1,2,1)\n","plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n","plt.legend(['loss', 'val_loss'])\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss [CrossEntropy]')\n","\n","plt.subplot(1,2,2)\n","plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n","plt.legend(['accuracy', 'val_accuracy'])\n","plt.ylim([0, 100])\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy [%]')"],"metadata":{"id":"2g5bKI2xK4jD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Đánh giá hiệu quả mô hình\n","Run the model on the test set and check the model's performance:"],"metadata":{"id":"je_NFU9rK9GG"}},{"cell_type":"code","source":["model.evaluate(test_spectrogram_ds, return_dict=True)"],"metadata":{"id":"hgDmtXwoK5Ss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Display a confusion matrix"],"metadata":{"id":"afNBquSgLFca"}},{"cell_type":"code","source":["y_pred = model.predict(test_spectrogram_ds)"],"metadata":{"id":"NiH1tV10LLHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = tf.argmax(y_pred, axis=1)"],"metadata":{"id":"sFDVShKxLYEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"],"metadata":{"id":"IzrcEUdTLZh6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(confusion_mtx,\n","            xticklabels=label_names,\n","            yticklabels=label_names,\n","            annot=True, fmt='g')\n","plt.xlabel('Prediction')\n","plt.ylabel('Label')\n","plt.show()"],"metadata":{"id":"gwvJHP3zLbfK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chạy suy luận trên tệp âm thanh"],"metadata":{"id":"TrMTlBZ-LlMl"}},{"cell_type":"code","source":["x = data_dir/'/content/drive/MyDrive/VOICE/newdata/Nam/audio37_converted.wav'\n","x = tf.io.read_file(str(x))\n","x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n","x = tf.squeeze(x, axis=-1)\n","waveform = x\n","x = get_spectrogram(x)\n","x = x[tf.newaxis,...]\n","\n","prediction = model(x)\n","x_labels = ['Bac','Trung','Nam']\n","plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n","plt.title('Bac')\n","plt.show()\n","\n","display.display(display.Audio(waveform, rate=16000))"],"metadata":{"id":"LjGQ4Mk-Llzw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Export the model with preprocessing"],"metadata":{"id":"VM3mOeivLxuv"}},{"cell_type":"markdown","source":["Mô hình này không dễ sử dụng nếu bạn phải áp dụng các bước tiền xử lý đó trước khi chuyển dữ liệu sang mô hình để suy luận. Vì vậy, hãy xây dựng một phiên bản end-to-end:"],"metadata":{"id":"_Q6aEsIEMGS7"}},{"cell_type":"code","source":["class ExportModel(tf.Module):\n","  def __init__(self, model):\n","    self.model = model\n","\n","    # Accept either a string-filename or a batch of waveforms.\n","    # YOu could add additional signatures for a single wave, or a ragged-batch.\n","    self.__call__.get_concrete_function(\n","        x=tf.TensorSpec(shape=(), dtype=tf.string))\n","    self.__call__.get_concrete_function(\n","       x=tf.TensorSpec(shape=[None, 16000], dtype=tf.float32))\n","\n","\n","  @tf.function\n","  def __call__(self, x):\n","    # If they pass a string, load the file and decode it.\n","    if x.dtype == tf.string:\n","      x = tf.io.read_file(x)\n","      x, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n","      x = tf.squeeze(x, axis=-1)\n","      x = x[tf.newaxis, :]\n","\n","    x = get_spectrogram(x)\n","    result = self.model(x, training=False)\n","\n","    class_ids = tf.argmax(result, axis=-1)\n","    class_names = tf.gather(label_names, class_ids)\n","    return {'predictions':result,\n","            'class_ids': class_ids,\n","            'class_names': class_names}"],"metadata":{"id":"6tUbxq4aLpyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test run the \"export\" model:"],"metadata":{"id":"EZNqmKMQMLKX"}},{"cell_type":"code","source":["export = ExportModel(model)\n","export(tf.constant(str(data_dir/'/content/drive/MyDrive/VOICE/newdata/Nam/audio37_converted.wav')))"],"metadata":{"id":"KOD_34-jMQCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.saved_model.save(export, \"saved\")\n","imported = tf.saved_model.load(\"saved\")\n","imported(waveform[tf.newaxis, :])"],"metadata":{"id":"TNQ_XfrzMSY8"},"execution_count":null,"outputs":[]}]}